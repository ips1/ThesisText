\chapter{Selecting functions to run}
\label{chap:deciding}

The key purpose of the ScalaAdaptive framework is to always invoke a function that is expected to have the best performance possible in given environment and with given inputs. By \textit{function performance} we mean any measurable description of the qualities of the function. It can be the actual run time of the function, memory consumption, number of I/O operations, number of threads created, execution counts for various parts of the code, etc. All these factors can be valuable and be the goal of the system run optimization.

This thesis is focused on the task of optimizing the function run time or time complexity in general. The core of the framework, however, is designed to be extensible by modules that can analyze the function run in different ways. More on this topic will be covered in chapter \ref{chap:implementation}.

\section{Overview of the selection process}
\label{sec:selection_overview}

Suppose we have functions $f_1, ..., f_n$, all of them doing the same thing, and an input $in$. We need to decide, which of the functions to select and invoke. To be able to do that, we need to formulate predictions about run times of the functions under given circumstances, and use the function with the lowest prediction. As the internal structure of the functions is unknown to us, the only data we can base the prediction on is the information we collect during the actual execution of the individual functions, stored in a history storage.

\subsection{Problems of the prediction and selection process}

There are three main factors we should take into account when predicting run times and using them to select a function:

\begin{itemize}
	\item The run times may depend on the input $in$
	\item The run times may depend on the environment
	\item The predictions are never precise, they have certain probability based on the amount and the character of the historical data
\end{itemize}

The \textit{input dependency} problem is being solved in many works about the program execution time predictions. Certain features of the input are analyzed and a model is constructed describing the relation to the run time. The model then can be used to predict the times for new inputs. This approach was used in \cite{chun_mantis:_2010,goldsmith_measuring_2007}. The authors of \cite{smith_predicting_1998} propose clustering the inputs prior to creating any models.

The \textit{environment dependency} is more complicated - there are no factors that we could watch and use to classify the runs or to include in the prediction model. The simplest solution is to give the history records a limited duration after which they expire, as the environment changes are not expected to occur very often, making the most recent data the most suitable for modeling. This technique is suggested among the adaptive framework use-cases in \cite{bulej_performance_2012}.

As for the \textit{uncertainty} problem, works in this field tend to use statistical methods to express the confidence of the prediction. In \cite{smith_predicting_1998}, confidence intervals are used for this purpose. The comparison of expectations used for selection should aways take these information into account and reject to select any function in case of uncertainty to prevent potentially wrong decision. Such a case should be solved by executing the function with least data to gather more observations.

\subsection{Input in the prediction process}
\label{subsec:input_in_selection}

The input, as mentioned earlier, can be a complex structure with multiple factors contributing to the run time\footnote{Consider for example a general graph - for most of the algorithms, both number of vertices and edges have to be taken into account}. Common prediction techniques (\cite{chun_mantis:_2010,goldsmith_measuring_2007}) use simplified model of the input $in$ represented by a vector of \textit{features} $f_1, ..., f_n$. 

Based on this approach, we will suppose that for each Scala function \(\exists g\) so that \(\forall in\) modeled by $f_1, ..., f_n$ holds \(t = g(f_1, ..., f_n) + \varepsilon\), where \(t\) is the runtime on input \(in\) of \inlinecode{f()}, and $\varepsilon$ is an error represented by a random variable with certain distribution, which we, for simplicity, assume to be normal. 

Now, the \textit{features} can be divided into 2 basic categories:

\begin{itemize}
	\item \textit{Features} with distance metrics
	\item \textit{Features} without distance metrics
\end{itemize}

If the \textit{feature} has distance metrics, i.e., is comparable (typically sizes of structures), we can construct models that use the it to analyze the trends and make predictions about inputs with feature values that were not processed before. The \textit{features} without the distance (usually enumerations and discrete configuration values) cannot be treated this way, and we need previous observations for given value before making any assumptions. 

To integrate these concepts into the framework with simplicity for the selection strategies, two mechanisms were included. First, to replace the vector of \textit{features} with distance metrics, a single integer called \textit{input descriptor} is used. The user can define a \textit{descriptor function} that extracts the \textit{input descriptor} from the input $in$ for given function. Then, at the beginning of the selection process, the \textit{descriptor function} is used and the resulting values is passed to the selecting strategy. In addition, the \textit{input descriptors} are stored in the run history as well, so a model can be built upon it and used in the selection process. We will suppose that \(\exists h\) so that \(\forall in\) with \textit{input descriptor} $x$ holds \(t = h(x) + \varepsilon\).

If we wanted to be more precise in the dependency modeling, we should be using a tuple of all the distance-based features as our \textit{input descriptor}. That would, however, lead to significantly more complex prediction models, and therefore we decided to keep the framework prototype simple by limiting ourselves to one factor. More complex models working with multiple features can be added to the framework in the future.

As for the \textit{features} with no metrics, the simplest way of addressing the issue is to treat different combinations of these \textit{features} the same way as if they were different functions. They should have completely separate historical measurement records, and the strategies can build separate models for them, using the \textit{input descriptor} on top of that. A possibility of \textbf{grouping} the inputs was therefore added and will be discussed in a separate section.

\subsection{Input grouping}
\label{subsec:grouping}

In \cite{smith_predicting_1998}, an automatic method of clustering the inputs is used before formulating any predictions about the run time. This is done because of the fact that inputs with common factors are in general expected to lead to similar run times, or at least to run times that can be described by the same, simpler model. In addition, it might be necessary to split the inputs using the values of \textit{input features} with no distance metrics, as explained in \ref{subsec:input_in_selection}.

Due to the performance limitations and the overall complexity of the solution, we will not include the algorithmic clustering. The user will, however, be able to specify a method of assigning the inputs to \textit{groups}. Upon selection, only the historical records from the corresponding group will be used.

\subsubsection{Group selection}
\label{subsubsec:group_selection}

The group selection function that determines a group for a given input is called a \textit{group selector} and has to be provided by the user when creating the combined function. Groups are identified by a custom type that can be either an integer, or a special \inlinecode{NoGroup} value. 

The grouping should be primarily based on the discrete values that affect the function behavior - enumerations, boolean flags and similar, as these cannot be included in the \textit{input descriptor} based model of the selection strategies. It can, optionally, include the \textit{features} covered by \textit{input descriptor} as well, in order to divide the range of all its possible values into more smaller ranges, which can increase precisions of the models. Some of the most common ways how to group values for the \textit{input descriptors} might be:
\begin{itemize}
	\item Logarithmic - orders of magnitude
	\item Linear - tens, hundreds, thousands
	\item Fixed - predefined finite number of groups
\end{itemize}

The groups in general should not be too small, as the selection process relies on only the data from the group. The advantage of groups with growing size (e.g. the logarithmic grouping) is that there is a lot of groups in the areas where the functions tend to compete in the performance (smaller inputs).

\subsection{Limiting the history record age}
\label{subsec:limiting_record_age}

The $\varepsilon$ in the function $h$ from section \ref{subsec:input_in_selection} technically represents the influence of the execution environment. We suppose that it is a random variable from a normal distribution, whose parameters characterize the environment. If the environment changes, the distribution of the error changes as well. In such a case, we should not be using the observations from before and after the change together in the models that suppose the same distribution for all the observations (which is the majority).

Therefore, whenever we expect the environment to change often and the run times to fluctuate significantly (e.g. due to network traffic, computation nodes, etc.), we should use only the most recent history records in the selection process, which will limit the probability of model failures. Not only will this lead to more significant results of the statistical selection strategies, but it also means that the framework will be able to quickly adapt to a new environment in case that the function performances get dramatically different.

We have the historical run records marked with the timestamps. The user of the framework will, optionally, provide a maximum age of a record that should be taken into account for a \textit{combined function}. Upon invocation, the records that are older than this limit will be filtered from the history vector of each one of the functions. This should not lead to any significant overhead if the vectors preserve the ordering in which they are build, having the most recent data at the end.


\subsection{Selecting from multiple functions}

The decision goal is obvious when selecting from two functions - compare the predictions and select the one that is better with given significance, or refuse to select when there is not such a function. Upon selection among three or more functions, the situation becomes less clear. Ideally, we would like to be able to identify a function that is significantly better that each one of the remaining functions. The problem is that there might be a function that is significantly better that some of the remaining functions, but not better than all of them. This function cannot be selected, because we are not able to rule out the possibility that there is a faster option.

For simplicity, we will consider only the case where one function is better than all the others, the remaining results will be treated as ambiguous and none will be selected. This can, unfortunately, lead to a situation where we have two functions with equally good performance and one with worse performance, we are not able to make a decision and we keep cycling through all three functions.

\section{The invocation chain}
\label{sec:selection_and_invocation_process}

We have discussed the selection process using prediction techniques on a theoretical level. Now, we are going to have a look at the actual steps that have to be done upon invocation of a combined function. Suppose we have a combined function $f$ created using the API described in chapter \ref{chapter:api} by marking functions $f_1,...,f_n$ as interchangeable. We hold a new function (or a function-like object) that can be called, passing it an input $in$, and is expected to run one of the implementations $f_1,...,f_n$ with $in$ and to return a result $out$.

The basic steps required to do so are the following:

\begin{enumerate}
	\item Use the \textit{group selector} to find the group identifier $g$ and the \textit{descriptor function} to find the input descriptor $x$ for $in$ (if either of the functions is not defined, the corresponding variable will have a special \textit{undefined} value)
	\item Locate the history vectors $d_1,...,d_n$ of functions $f_1,...,f_n$ for group $g$
	\item Filter out records older than the maximum age (infinite by default) from $d_i$ into $d'_i$ $\forall i$
	\item Select the function $f_k$ to run based on $d'_1,...,d'_n$ and $x$
	\item Invoke $f_k$, fetch the result $out$ and evaluate its run time
	\item Add the new evaluation result to $d_k$ and update it for group $g$
	\item Return $out$ to the caller
\end{enumerate}

In the following sections, we will discuss some of the steps of the process in more details.

\subsection{Run time evaluation}

When talking about function run time (or an execution time of a function), there are two types of values that we could be observing:

\begin{itemize}
	\item \textbf{\textit{Wall clock time}} - time elapsed between entering and leaving the function
	\item \textbf{\textit{CPU time}} - time that the CPU actually spent executing our function
\end{itemize}

The \textit{wall clock time} is always higher than the \textit{CPU time}, because it includes not only the time when CPU is executing the function code, but also the time when the executing thread is waiting for its turn in time-sharing multitasking OS or sleeping on a blocking I/O\footnote{Input / Output operation} operation, synchronization primitive, or for some other reason. This means that the \textit{wall clock time} also gets affected by concurrently running processes, network load, and other environment-based factors, and tends to vary much more between multiple invocations.

For the purposes of the ScalaAdaptive framework, it might seem that the \textit{CPU time} would be more appropriate, as it gives clearer results not affected by the state of the executing environment. The truth is, however, that many of the use cases of the framework require the thread sleeping time to be included in the measurement, as the functions run times are determined mainly by the duration of an I/O operation (e.g. database queries, network requests, etc.), so using \textit{CPU time} would not give us the necessary results.

It is also quite difficult to determine the \textit{CPU time} - it requires support from the operating system with tracking the time that every thread has spent in execution. For this reason and for the reason stated above, the selection process in this text is based on the \textit{wall clock time}. It might be, however, interesting as a future extension to implement measuring of both of the times and allowing the user to decide for each combined function which time should be used.

The \textit{wall clock time} is measured by fetching high-precision system time in nanoseconds right before calling the function apply method and right after returning from the call. The result is the run time in nanoseconds. This measurement is precise enough for all the use cases of the framework, because it is targeting mainly functions with non-negligible time complexities.

The evaluation data might potentially contain more information that can help with classifying the function run and make better predictions in the selection process, e.g. the data about execution counts of certain code paths of the function, like in \cite{chun_mantis:_2010}. That would, however, require instrumentation of the executed code, and was not implemented in this work for its complexity.

\subsection{Storing and retrieving the evaluation data}

After having invoked the function and evaluated its run, the evaluation data have to be stored before passing the return value back to the caller. The history of all the run evaluations is kept together for each combination of a function and a group identifier as a vector of records. Each record consists of at least the following:

\begin{enumerate}
	\item The \textit{input descriptor} of the input for given run
	\item The evaluation metrics (in our case, the run time) of given run
	\item The timestamp of given run
\end{enumerate}

Whenever a new evaluation data is obtained, a record is created and added to the end of the history vector. Some additional metrics might be cached for the whole history record, in such a case, the metrics have to be updated as well.

For simplicity, we suppose that the run histories of the actual implementations will be shared across all the combined functions that use them. More on that along with different implementation options will be discussed in chapter \ref{chap:implementation}.

\subsection{Selecting a function}

The actual selection process, as described in \ref{sec:selection_overview}, can be implemented in many ways. It is basically an isolated algorithm with the following input:
\begin{enumerate}
	\item History data vectors $d_1,...,d_n$ of functions $f_1,...,f_n$, where $d_i = [(x_{i,1}, y_{i,1}),...,(x_{i,l_i}, y_{i,l_i})]$ are the \textit{input descriptors} and corresponding run times of historical runs (the \textit{input descriptors} can be \textit{undefined})
	\item Current input descriptor $x$
\end{enumerate}

It should return $k \in {1,...,n}$, which will determine the function $f_k$ to run.

These algorithms are called \textit{selection strategies}, and we suggested a few of them based on different approaches to predicting the run times and selecting between various predictions. They will be described in the following two sections, and for each that we included in the framework, a simplified pseudocode implementation will be included at the end of the description.

\section{Mean based selection strategies}
\label{sec:mean_based_strategies}

In some cases, the function run time does not depend on the input, or the relation is not significant enough. For simplification, we suppose that the function complexity is constant in such a case. Using the same formalism as in \ref{subsec:input_in_selection}, for run time $t$ on input $in$ with \textit{input descriptor} $x$ holds \(t = h(x) + \varepsilon\) for a $h$ constant, i.e. $\forall y$ $g(y) = c$. Therefore, $t = c + \varepsilon$, where $\varepsilon$ is the error which we suppose has a normal distribution with a zero mean. Under these assumptions, $t$ is a random variable with normal distribution and mean equal to $c$, which is the expected constant run time of the function.

The \textit{mean based} strategies will use the sample data to determine the means of the function time distributions with given certainty and use them as the expected run times in the decision process. The \textit{input descriptor} $x$ will be ignored in the process, both the ones included in the historical measurements and the current one. Therefore, it will not be mentioned in the pseudocode algorithm input.

\subsection{T-test for two functions}
\label{subsec:t_test_two}

To compare means of two samples from certain distribution and make conclusion about their equality, statistical tests are commonly used.

Suppose we have two samples of run times for the two functions involved, $X_1, ..., X_n$ and $Y_1, ..., Y_m$. Next, suppose that these samples come from normal distributions with means $\mu_1$ and $\mu_2$, respectively. These means are unknown for us, just like the variances, which we suppose to be different. The goal is to test the means of the two samples against each other.

Based on these requirements, we will use a \textit{two-sample t-test}. Standard Student's t-test works only under the assumption that the sample variances are equal, so it cannot be used in this case. We are going to use the generalized version, the Welch's (nonpooled) t-test \cite{welch_generalisation_1947}, which is not as strong, but can work with samples with different variances. More detailed reasoning about the test selection can be found in \cite{weiss_introductory_2010}.

The default hypotheses for the test are:

\[
H_0: \mu_1 = \mu_2
\]
\[
H_1: \mu_1 \ne  \mu_2
\]

and the test statistics used:

\[
T = \frac{\bar{X}_n - \bar{Y}_m}{S}
\]

where $\bar{X}_n$ and $\bar{Y}_m$ are the sample means and $S$ is the following:

\[
S = \sqrt{\frac{S_X^2}{n} + \frac{S_Y^2}{m}}
\]

with $S_X^2$ and $S_Y^2$ being the sample variances:

\[
S_X^2 = \frac{1}{n-1} \sum_{i=1}^{n}(X_i - \bar{X}_n)^2
\]
\[
S_Y^2 = \frac{1}{m-1} \sum_{i=1}^{m}(Y_i - \bar{Y}_m)^2
\]

This test statistics can be estimated according to \cite{weiss_introductory_2010} using Student's t-distribution with $df$ degrees of freedom for:

\[
df = \frac{(\frac{S_X^2}{n} + \frac{S_Y^2}{m})^2}{\frac{S_X^4}{n^2(n - 1)} + \frac{S_Y^4}{m^2(m - 1)}}
\]

The hypothesis $H_0$ will be rejected in favor of hypothesis $H_1$ with the significance level $\alpha$ if

\[\abs{T} > t_{df}(1-\frac{\alpha}{2})\]

The $t_{df}(1-\frac{\alpha}{2})$ is the $(1-\frac{\alpha}{2})$-th quantile of \textit{t-distribution} with $df$ degrees of freedom.

Rejecting $H_0$ in favor of $H_1$ means that the sample distributions have significantly different expectations, i.e., one of the functions is expected to give better results than the other. We can then simply compare the sample means and determine, which of the functions has lower expected run time and should be selected to run with given certainty (the probability of $1-\alpha$). If we cannot reject the $H_0$, the strategy is not able to determine which function is better with given certainty and should not give a result.

\subsubsection{Selection algorithm}

The strategy is parametrized by the selection significance level $\alpha$.

\begin{algorithmic}[1] % The number tells where the line numbering should start
	\INPUT Historical data vectors $d_1,d_2$ where $d_i = [y_{i,1},..., y_{i,l_i}]$
	\State $equalityRejected \gets$ twoSidedTTest($d_1, d_2, \alpha$)
	\If{not $equalityRejected$}
	\State \Return{useSecondarySelector($d_1,d_2$)}
	\EndIf
	\If {$mean(d_1) < mean(d_2)$}
	\State \Return{1}
	\Else 
	\State \Return{2}
	\EndIf
\end{algorithmic}

\subsection{T-test for multiple functions}
\label{subsec:t_test_multiple}

The selection strategy that was described in \ref{subsec:t_test_two} works only with two functions. If we want to select from three or more, we need to modify the strategy.

Suppose we have $k$ samples $X_{1,1}, ..., X_{1, n_1}, ..., X_{k,1}, ..., X_{k, n_k}$ from normal distributions with unknown expectations $\mu_1, ..., \mu_k$ and unknown variances. We would like to compare the expectations and find out whether there is a sample with mean significantly different from the others.

Common techniques for testing hypotheses about the expectations of multiple samples involve \textit{ANOVA}\footnote{Analysis of variance} or its modified version, the \textit{Kruskal-Wallis test} (both described in \cite{weiss_introductory_2010}). The tests are based on variance comparison, assume that the samples have similar standard deviations, and can decide about the following hypotheses:

%TODO: Add references to the tests

\[
H_0: \mu_1 = \mu_2 = ... = \mu_k
\]
\[
H_1: \exists i, j: \mu_i \neq \mu_j
\]

These multiple-sample tests are usually preferred over performing multiple t-tests, because in case of the t-tests, it is necessary to correct the significance level by artificially lowering the $\alpha$ of each of the tests, which leads to lower test strengths. In our case, however, this approach would have a few problems. Firstly, we want to actually find out which of the samples is the deviating one, or, if there are more, which one is the most deviating one. Secondly, we need the one-sided alternative (to find out the lowest expectation). 

Based on these observations, it is necessary to come up with different hypotheses. We will consider a simpler task first - suppose we want to test $\mu_1$ against all the other expectations:

\[
H_0: \mu_1 = \mu_2 \vee ... \vee \mu_1 = \mu_k
\]
\[
H_1: \mu_1 < \mu_2 \wedge ... \wedge \mu_1 < \mu_k
\]

$H_0$ is basically saying that there is at least one other sample with the same mean, and the one-sided alternative $H_1$ puts $\mu_1$ as the lowest of all the means.

We can perform a simple t-test of $H_{j,0}: \mu_1 = \mu_j$ against $H_{j,1}: \mu_1 < \mu_j$ $\forall j = 2, ..., k$. It is clear that the $H_0$ can be rejected in favor of $H_1$ only if all of the $H_{j,0}$ were rejected in favor of $H_{j,1}$. Note that the $H_{j,1}$ is an alternative hypothesis of a one-sided test - we need to take it into account when performing the test and modify the critical values.

As mentioned before, the procedure of combining statistical tests usually requires artificially lowering the significance level of the individual tests with the goal of keeping the significance level of the whole set of tests reasonably low. These methods are based on the assumption that committing a type-I error in any of the individual tests leads to a type-I error overall, and the probability of committing at least one gets higher with the number of tests.

In our case, however, a type-I error in one of the individual tests does not necessarily lead to a type-I error in the whole test. Suppose that $H_0$ is true, so for an $l \geq 1$ $\exists i_1,...,i_l: \mu_1 = \mu_{i_1} \wedge ... \wedge \mu_1 = \mu_{i_l}$. In order to commit a type-I error, we need to reject $H_{i_j,0}$ $\forall j = 1, ..., l$, by committing the type-I error on the individual tests $l$-times. At the same time, we need to decide correctly about the remaining $k-l$ tests. So the probability of committing a type-I error overall is $\alpha^l(1-\alpha)^{k-l}$, and consequentially, the significance level of the entire test gets even lower than $\alpha$.

As the $\alpha$ decreases with growing $k$, the test gets more pessimistic - it will require more data and more certainty in order to reject the $H_0$. If we wanted to keep the significance level at exactly $\alpha$, we would need to apply a reverse correction - artificially increase $\alpha$. The exact method how to do so would, however, be quite difficult to derive, as there are a lot of cases to consider with unknown probabilities. The assumption is that the number $k$ of the functions involved will be low enough not to affect the significance too much. 

Now, we are able to decide about a function whether it is significantly better than all of the others. In the selection strategy, this test will be performed for all the functions one by one and the first one where the $H_0$ is rejected in favor of $H_1$ gets selected. This means performing at most $k$ of these combined tests. In this case, the type-I error in any of the tests leads to a critical error of the selection process and we should try to keep it as unlikely as possible, by applying one of the correction techniques mentioned. A simple example of such a technique could be the \textit{Bonferroni correction}, which sets the $\alpha'$ of the individual tests to $\alpha / k$ for $\alpha$ being the desired significance overall and $k$ the number of samples.

%TODO: Bonferroni correction reference
%TODO: Teoria statistica delle classi e calcolo delle probabilità? :D

\subsubsection{Selection algorithm}

The strategy is parametrized by the selection significance level $\alpha$.

\begin{algorithmic}[1] % The number tells where the line numbering should start
	\INPUT Historical data vectors $d_1,...,d_n$ where $d_i = [y_{i,1},...,y_{i,l_i}]$
	\For{$i = 1$ to $n$}
		\State $better \gets true$
		\For{$j = 1$ to $n$}
			\State $equalityRejected \gets$ oneSidedTTest($d_i, d_j, \alpha$)
			\If{not $equalityRejected$ or $mean(d_i) > mean(d_j)$}
				\State $better \gets false$
			\EndIf
		\EndFor
		\If{$better$}
			\State \Return{$i$}
		\EndIf
	\EndFor
	\State \Return{useSecondarySelector($d_1,...,d_n$)}
	
\end{algorithmic}

\subsection{Mann–Whitney U test}

The t-tests used in strategies from sections \ref{subsec:t_test_two} and \ref{subsec:t_test_multiple} requires the historical measurement data to come from a normal distribution. Even though it was one of our assumptions at the beginning, it does not necessarily hold in reality, as the fluctuations in the run time depend on many factors of the environment. We might try to use a test that is nonparametric, i.e., does not require any standardized and parametrized distribution of the data. One of these tests is the Mann–Whitney U test (for details see \cite{weiss_introductory_2010}), which only requires the sample observations to be independent.

Supposing we have the samples $X_1, ..., X_n$ and $Y_1, ..., Y_m$ from distributions with respective means $\mu_1$ and $\mu_2$, the hypotheses are going to be exactly the same as for the t-test:

\[
H_0: \mu_1 = \mu_2
\]
\[
H_1: \mu_1 \ne \mu_2
\]

The test itself is based on ranking all the observation from both samples together by their values, and then adding up the ranks in the first sample ($R_1$) and in the second sample ($R_2$). The test statistics is the following:

\[U = min(U_1, U_2)\]

\[U_1 = R_1 - \frac{n(n+1)}{2}\]
\[U_2 = R_2 - \frac{m(m+1)}{2}\]

Now, U has, in case of valid $H_0$, a known distribution whose critical values can be either looked up for small fixed values, or approximated using a normal distribution for higher values. Therefore, we can reject the $H_0$ for $U < U(\alpha, n, m)$, where $U(\alpha, n, m)$ is the tabulated or approximated critical value for the significance level $\alpha$ and given sample sizes.

The strategy can be extended to three or more functions exactly in the same way as the t-test (see section \ref{subsec:t_test_multiple}), the individual tests involved will have to be changed to one-tailed (by using doubled $\alpha$) and an overall $\alpha$ correction should be used.

\subsubsection{Selection algorithm}


The strategy is parametrized by the selection significance level $\alpha$.

\begin{algorithmic}[1] % The number tells where the line numbering should start
	\INPUT Historical data vectors $d_1,...,d_n$ where $d_i = [y_{i,1},...,y_{i,l_i}]$
	\For{$i = 1$ to $n$}
	\State $better \gets true$
	\For{$j = 1$ to $n$}
	\State $equalityRejected \gets$ oneSidedUTest($d_i, d_j, \alpha$)
	\If{not $equalityRejected$ or $mean(d_i) > mean(d_j)$}
	\State $better \gets false$
	\EndIf
	\EndFor
	\If{$better$}
	\State \Return{$i$}
	\EndIf
	\EndFor
	\State \Return{useSecondarySelector($d_1,...,d_n$)}
	
\end{algorithmic}

\section{Input based selection strategies}
\label{sec:input_based_strategies}

\textit{Input based strategies} are the strategies that use the historical measurements along with corresponding \textit{input descriptor} to construct a model that approximates the function $h$ (see \ref{subsec:input_in_selection}). A prediction for a new \textit{input descriptor} can be derived from the model and used to decide which of the functions will have better performance on given input.

\subsection{Simple linear regression}
\label{subsec:simple_linear_regression}

The first strategy supposes that the function \(h\) can be approximated precisely enough using a linear function \(h'\) in the following form:
\[h'(x) = a' x + b'\]

This approximation is a \textit{simple linear regression model}. The slope \(a'\) and intercept \(b'\) of the linear function can be determined using a least squares method, which minimizes the squares of the distances of the actual function values from the ones predicted by the model.

A big advantage of the model is its simplicity and thus minimal overhead when generating the regression during the selection process. In addition, the model can be easily cached for fixed set of samples and updated when adding new data without having to construct it all over again.

The main problem is that the linear regression model applied to a case where the relation is not linear leads to a non-trivial errors that increase with the range that we are trying to cover with the model. Figure \ref{fig:selection_sort_linear_trendline} shows samples of run times of the basic selection sort algorithm, which has the complexity of \(O(N^2)\), on sample arrays with 0 to 30000 integers. Collected data obviously match the quadratic function graph marked with the green color. The red line shows the linear regression model. As we can see, the predictions based on this model would be quite inaccurate, especially if we tried to predict run time on an input significantly larger than the samples.

\begin{figure}[h!]
	\captionsetup{justification=centering,margin=0.5cm}
	\centerline{\mbox{\includegraphics[width=90mm]{./img/selection_sort_linear_trendline.png}}}
	\caption{Linear regression line (red) of a selection sort runtime samples.}
	\label{fig:selection_sort_linear_trendline}
\end{figure}

Next, we need to find a way how to select the best function with given certainty using a set of these models.

\subsubsection{Confidence interval based selection from two functions}
\label{subsubsec:confidence_interval_selection}

Suppose we have a linear regression model \(h'(x) = a' x + b'\) of a function $h(x) = ax + b$ constructed using \(n\) data samples \(x_i, y_i\) (in our case, \(x_i\) are the input descriptors and \(y_i\) are the run times). If we assume that the errors $\varepsilon = y_i - h'(x_i)$ of our sample from the regression model values are random variables with normal distribution, we can perform some observation on the model.

Firstly, the standard errors $se(b')$ and $se(a')$ of the intercept $b'$ and the slope $a'$ can be derived from the model. In addition, the statistics $t_a = \frac{a' - a}{se(a')}$ and $t_b = \frac{b' - b}{se(b')}$ have a t-distribution with $n-2$ degrees of freedom. This means that we are able to construct confidence intervals and perform t-tests with $a$ and $b$. The default tests hypotheses have the following form:

\[H_0: a = a_0\]
\[H_1: a \neq a_0 \]

We basically test equality of $a$ (or $b$) and a fixed value $a_0$ (or $b_0$) - this test can be used to verify if the data have a specific linear relation. If we put $a_0 = 0$, we can whether the observations $y_i$ are constant. 

None of these tests and statistics, however, allows us to test the certainty of the prediction - we need to consider both the intercept and the slope estimation certainty, and, in addition, the distance of the $x$ we want the prediction for from the mean $\bar{x}$, because if we commit an error in the slope estimation $a'$, the corresponding error in predictions increases with the distance from $\bar{x}$. 

With these facts, a confidence interval on level $1 - \alpha$ for $h'(x)$ can be constructed (specifically for each point $x$). According to \cite{weiss_introductory_2010}, the width of the interval is the following:

%TODO: Change the text to "mean prediction interval"

\[
w_{x, \alpha} = t_{n - 2}(1-\frac{\alpha}{2}) s_e\sqrt{\frac{1}{n} + \frac{(x - \bar{x})^2}{ \sum_{i = 1}^{n} (x_i - \bar{x})^2 }}
\]

where \(\bar{x}\) is the mean of \(x_i\), $t_{n - 2}(1-\frac{\alpha}{2})$ is the $(1-\frac{\alpha}{2})$-th quantile of \textit{t-distribution} with $n-2$ degrees of freedom, and $s_e$\footnote{Standard error of the estimate} can be computed as:

\[s_e = \sqrt{\frac{SSE}{n-2}}\]

And:

\[SSE = \sum_{i = 1}^{n} \varepsilon_i^2  =  \sum_{i = 1}^{n} (y_i - y_i')^2 \]

for \(y_i' = h'(x_i)\).

It now holds that for any $x$, the corresponding $y \in [a'x + b' \pm w_{x, \alpha}]$ with probability $(1-\alpha)$.

Now in the situation where we have two regression models and we want to find the best prediction of $x$, we can construct these confidence intervals for both models and:

\begin{itemize}
	\item If the intervals do not overlap, the predictions have at least a $(1 - \alpha)^2$ probability of correctly setting the order, i.e. the worse prediction actually being the worse function
	\item If the intervals overlap, there is a non-trivial chance of the predictions being mixed up by error
\end{itemize}

In the first case, we can decide that one function is significantly better and select it. In the second case, we cannot decide with given certainty.

\subsubsection{Confidence interval based selection from multiple functions}

The described method allows us to decide between two functions with given certainty. Now, when choosing among $n$ functions with regressions $h'_1, ..., h'_n$ for given $x$, we are looking for the $k$ for which the following holds:

\begin{itemize}
	\item The confidence interval on level $(1-\alpha)$ for prediction $h'_k(x)$ does not overlap with any other of the confidence interval for prediction.
	\item $\forall j \ne k: h'_k(x) < h'_j(x)$
\end{itemize}

It can be done by constructing all the regression models and then $\forall i$ performing the comparison described in \ref{subsubsec:confidence_interval_selection} of $h_i$ with $h_j, \forall j \ne i$. If $h_i$ is better than all of the $h_j$, we can choose the corresponding function.

The strength of this decision is influenced by the fact that we perform multiple confidence interval constructions - the errors an error in any of them can influence the result of the selection, se the significance gets lower in general. If we wanted the results to be precise, we would have to perform the correction of the significance level $\alpha$.

%TODO: How to set alpha? correction?

\subsubsection{Selection algorithm}

The strategy is parametrized by the selection significance level $\alpha$.

\begin{algorithmic}[1] % The number tells where the line numbering should start
	\INPUT Historical data vectors $d_1,...,d_n$ where $d_i = [(x_{i,1}, y_{i,1}),...,(x_{i,l_i}, y_{i,l_i})]$, input descriptor $x$
	\For{$i=1$ to $n$}
	\State $r_i \gets$ createRegression($d_i$) \Comment{Uses the least squares method}
	\EndFor
	\For{$i = 1$ to $n$}
	\State $better \gets$ true
	\For{$j = 1$ to $n$, $j \ne i$}
	\State $(k_1, l_1) \gets$ predictionInterval($r_i, x, \alpha$)
	\State $(k_2, l_2) \gets$ predictionInterval($r_j, x, \alpha)$
	\If{$(k_1, l_1)$ overlaps $(k_2, l_2)$ or $k_1 > k_2$}
	\State $better \gets$ false
	\EndIf
	\EndFor
	\If{$better$}
	\State \Return{$i$}
	\EndIf
	\EndFor
	\State \Return{useSecondarySelector($d_1,...,d_n, x$)}
\end{algorithmic}

\subsection{Window-bound linear regression}
\label{subsec:window_bound_regression}

To be able to apply the strategy described in section \ref{subsec:simple_linear_regression} on a wider range of functions, a simple approach to lower the approximation errors can be taken. Instead of creating a regression model for the entire set of historical measurements at once, we can split it into more subsets and create different regressions for them. Figure \ref{fig:window_examples} shows ranges $5000 - 10000$, $10000-15000$ and $15000-20000$ of the input sizes of the data from \ref{fig:selection_sort_linear_trendline}. As we can observe from the second and third range, the data tend to be much closer to the linear regression line. The first range shows that a less significant fluctuation in the entire set has a lot bigger impact on the model in a smaller subset.

\begin{figure}[h!]
	\captionsetup{justification=centering,margin=0.5cm}
	\centerline{\mbox{\includegraphics[width=140mm]{./img/window_examples.png}}}
	\caption{Examples of subsets of the data from figure \ref{fig:selection_sort_linear_trendline} with linear regressions.}
	\label{fig:window_examples}
\end{figure}

When selecting a function for a specific \textit{input descriptor} $d$, a window around its value of specific width $w$ can be created and the regression model built using the data from said window. We do it by selecting only the historical samples $x_k, y_k$ where $\abs{x_k - d} < w$ when constructing the regression.

There are multiple ways how to decide on the window width $w$. Using a fixed number across all the functions and all the data samples would not be recommendable, as the functions can vary in their \textit{input descriptors} by orders of magnitude. The width could be derived as a fraction of the width of the whole data set: $w = \frac{max(x_i) - min(x_i)}{p}$, where $p$ would represent in how many parts we basically want it to be split. 

More flexible approach is to dynamically determine the $p$. Suppose we want the average window to contain $r$ records. We find out what is the average distance between two neighboring records as $d =  \frac{max(x_i) - min(x_i)}{n - 1}$ and then set the $p = r * d$. This way, the window size will always be adapted to the density of the dataset.

The window limitation might seem similar to the grouping described in \ref{subsec:grouping}, but there are two major differences. First, the windows are based on the \textit{input descriptor}, whereas the \textit{group selector} might use different features of the input. And second, the groups are deterministic and do not move with the \textit{input descriptor} and change their size based on historical data. Both these concept can be used at the same time.

\subsubsection{Selection algorithm}

The strategy is parametrized by the selection significance level $\alpha$ and an average record count per window $avg$.

\begin{algorithmic}[1] % The number tells where the line numbering should start
	\INPUT Historical data vectors $d_1,...,d_n$ where $d_i = [(x_{i,1}, y_{i,1}),...,(x_{i,l_i}, y_{i,l_i})]$, input descriptor $x$
	\For{$i=1$ to $n$}
	\State $dist \gets (max(x_{i,j}) - min(x_{i,j})) / l_i$
	\State $w \gets dist * avg$
	\State $d'_i \gets [(x_{i,j}, y_{i, j}); \forall j: \abs{x_{i,j} - x} \leq w/2]$
	\EndFor
	\State \Return{useLinearRegressionStrategy($d'_1,...,d'_n, x$)}
\end{algorithmic}

\subsection{Local regression}
\label{subsec:local_regression}

The approach explained in \ref{subsec:window_bound_regression} can be extended further - the locally constructed linear regression models might be used to build up a model for the entire data set. This leads to a model that is non-linear, i.e., the function that approximates the original data relation is not a linear function.

One possible regression method that behaves this way is \textit{LOESS} (Local Regression, \cite{cleveland_locally_1988,cleveland_regression_1988,cleveland_computational_1991}). It uses methods similar to the least-square regression on local subsets of the data set, and then smooths up the resulting curve. The output is a \textit{Loess Curve}, an artificially constructed function that does not necessarily have to be expressed by a formula. This is one of the advantages of the model - the original relation between \textit{input descriptors} and the run times did not have to be expressed by a nice simple function in order to be captured correctly.

The \textit{LOESS} model can be used to predict the function run times. It is, however, a lot more complicated to derive the prediction confidence intervals. Therefore, only the values will be used in this selection strategy, which could lead to some wrong decisions.

There are other disadvantages of this strategy as well. First of all, the entire local regression model has to be recomputed whenever there is a new record in the dataset, the model cannot be simply updated like the \textit{simple linear regression}. Secondly, it is much more computationally complex and the model construction takes non-trivial time, so it has a negative impact of the framework overhead, especially when used with short and simple functions. The last downside of \textit{local regression} is its inability to predict further behavior past the minimum and maximum data sample.

\subsubsection{Selection algorithm}

\begin{algorithmic}[1] % The number tells where the line numbering should start
	\INPUT Historical data vectors $d_1,...,d_n$ where $d_i = [(x_{i,1}, y_{i,1}),...,(x_{i,l_i}, y_{i,l_i})]$, input descriptor $x$
	\For{$i=1$ to $n$}
	\State $m_i \gets$ buildRegression($d_i$)
	\State $p_i \gets$ predictValue($m_i, x$)
	\EndFor
	\State\Return{$argmin(p_i)$}
\end{algorithmic}

\subsection{Window-bound t-test}

The mean based t-test selection strategy described in sections \ref{subsec:t_test_two} and \ref{subsec:t_test_multiple} works under the assumption that the run times of a specific function have a normal distribution, and the prediction of run time is simply the mean of all the times measured. This could be the truth only for the functions with constant complexity - whenever the function run time depends on the input, the samples measured have their distribution influenced by the distribution of the inputs.

\begin{figure}[h!]
	\captionsetup{justification=centering,margin=0.5cm}
	\centerline{\mbox{\includegraphics[width=90mm]{./img/quick_vs_selection.png}}}
	\caption{Run times of quick sort algorithm (blue dots) and selection sort algorithm (red dots) by input size.}
	\label{fig:quick_vs_selection}
\end{figure}

\begin{figure}[h!]
	\captionsetup{justification=centering,margin=0.5cm}
	\centerline{\mbox{\includegraphics[width=110mm]{./img/window_t_test_examples.png}}}
	\caption{Examples of subsets of the data from figure \ref{fig:quick_vs_selection}}
	\label{fig:window_t_test_examples}
\end{figure}

If we wanted to use the t-test strategy on the functions with non-constant complexity, the same approach as with the \textit{simple linear regression} can be taken - we can perform the t-test just for a subset of the historical data with very similar \textit{input descriptors}. Instead of assuming that the relation in the small window is linear, we want it to be constant and normally distributed. Consequently, even smaller window sizes should be selected, so that these assumptions held at least approximately. The t-test significance levels and strengths will be approximate as well, but our goal is to get reasonable results, we do not need to be precise.

The figure \ref{fig:quick_vs_selection} shows the run times measured for a quick sort and selection sort algorithms on random arrays of sizes $0-30000$. Obviously, there is a relation to the input size, so the simple t-test would not help us in this case. In figure \ref{fig:window_t_test_examples}, there are the same data in three different windows - $0-3000$, $7000-10000$ and $17000-20000$. As we can see, in a window of this size, the run times are almost constant and the random fluctuations are more significant than the actual relation to the input size. We can apply the t-test as described in sections \ref{subsec:t_test_two} and \ref{subsec:t_test_multiple} to these windows and use the result as relevant enough with respect to the input.

The window sizes can be determined in a similar way as in section \ref{subsec:window_bound_regression}. If we reduce it to a constant 0, we will get a technique in which only the results for inputs with the same descriptor are tested. When there are none, we refuse to select and wait until data is gathered. This might be useful in cases where we know that the function will be called repeatedly on a limited set of input variants, and basically rules out the prediction factor, as we need to have actual observation for each input before making any conclusions. Similar approach is taken in \cite{bulej_performance_2012}.

\subsubsection{Selection algorithm}

The strategy is parametrized by the selection significance level $\alpha$ and an average record count per window $avg$.

\begin{algorithmic}[1] % The number tells where the line numbering should start
	\INPUT Historical data vectors $d_1,...,d_n$ where $d_i = [(x_{i,1}, y_{i,1}),...,(x_{i,l_i}, y_{i,l_i})]$, input descriptor $x$
	\For{$i=1$ to $n$}
	\State $dist \gets (max(x_{i,j}) - min(x_{i,j})) / l_i$
	\State $w \gets dist * avg$
	\State $d'_i \gets [(x_{i,j}, y_{i, j}); \forall j: \abs{x_{i,j} - x} \leq w/2]$
	\EndFor
	\State \Return{useTTestStrategy($d'_1,...,d'_n$)}
\end{algorithmic}

\subsection{Whitebox model construction}

The methods that were described so far are examples of the blackbox techniques - they do not analyze the function itself, the models are based only on the relation of the run times and the \textit{input descriptors}. 

The techniques used in the research projects focused on formulating accurate predictions about program run times (like \cite{goldsmith_measuring_2007,chun_mantis:_2010,huang_predicting_2010}) often build the model using more detailed information about every run. These extra data can be obtained by examining and instrumenting the function code. Key structures (loops, branches, method invocation, ...) can be identified and an execution counts can be tracked. Then, a model that captures the relation between the input features, the described structure execution counts and the run time. This added dimension might lead to more accurate model and therefore better predictions.

%TODO: Add reference / bibliography
In \cite{chun_mantis:_2010}, a Mantis framework for high accuracy program performance prediction was implemented. The basic principles are similar to the approach described above. It was tested on a series of data gathered from runs of the ImageJ application, and compared with a blackbox prediction model based on interpolating a polynomial of degree 3. The prediction error of the whitebox approach was 5.5\%, the blackbox approach error rate reached more than 35\%.
%TODO: Add results?

This approach has, however, a few problems concerning the intended use cases of ScalaAdaptive framework. The model will not improve the predictions if the majority of the execution time is spent waiting on an I/O operation. Specifically, it will not help with any of the cases where we are selecting a database query, remote server to connect to, etc. In addition, it would require the method code to be instrumented at runtime, which would lead, together with the model construction, to a significant overhead.

For these reasons and because of the complexity, the whitebox model was not implemented in the framework, but it represents a potential future work that could be done on the topic.

\section{Strategy comparison}
\label{sec:strategy_comparison}

In general, the input based strategies do not handle well more historical results for the same \textit{input descriptor}, and are impossible to use when there is none specified. The following rules should be used to decide between input based and mean based strategies:

\begin{itemize}
	\item \textbf{Dependency on the input} \\
	If the function does not have any input, or the input does not affect the expected run time (the complexity is constant), mean based strategies should be used.
	\item \textbf{Expected input values} \\
	If the function is expected to be invoked with a discrete and very limited set of inputs, mean based strategy is preferred. On the other hand, if the function will be called repeatedly with randomly distributed inputs, the input based strategies are a better choice.
\end{itemize}

Considering these facts, the framework will contain both a input based and a mean based strategy, and will automatically use the input based one if the user specified an \textit{input descriptor}, and the mean based one otherwise. The user will be able to override this choice if he wants.

The actual strategies used should be easily replaceable, but some have to be chosen as default for the common usages. As we introduced only one mean based strategy independent on the input, there is not much of a choice. The situation is more complicated In the field of input based strategies, and we need to perform a couple of tests.

\subsection{Input based strategies success rates}

To compare the efficiency of the selection strategies, we used an artificial scenario with two functions. One with linear complexity that computes a sum of $n$ random numbers for an input $n$. The other computes a sum of $n * k$ random numbers for an input $n$, where $k$ is an \textit{error factor} determining how much slower the function is. We would expect the selection strategy to always pick the first function for any $k>1$, so we will track the number of wrong decisions (i.e. number of times the strategy selects the second function) for the three input based strategies and for various \textit{error factor} values.

We will use the values 4, 2, 1.5, 1.2 and 1.1 for $k$ and for each of them generate 5 sequences of 200 inputs. Then, we will use all the selection strategies on every sequence, removing all the historical records in between. There is a minimum of 30 records in the function history before the decision process starts, so the actual decision will take place only on 140 inputs in each sequence.

The results for the described text after adding up the wrong decision numbers for all 5 sequences can be found in table \ref{tab:strategy_comparison}. The numbers shown are the numbers of times the slower function was selected out of 700 in total, along with corresponding percentage. The strategies used are the linear regression (LR) with $\alpha = 0.05$, window-bound linear regression with $\alpha = 0.05$ and the local regression (LOESS).

\begin{table}[h!]
	\captionsetup{justification=centering,margin=0.5cm}
	\centering
	\bgroup
	\def\arraystretch{1.5}%
	\begin{tabular}{|r|r|r|r|}
		\hline
		\multicolumn{1}{|l|}{} & \multicolumn{1}{c|}{\textbf{LR}} & \multicolumn{1}{c|}{\textbf{Window-bound LR}} & \multicolumn{1}{c|}{\textbf{LOESS}} \\ \hline
		\textbf{4x}            & 107 (15.3\%)                     & 35 (5.0\%)                                    & 156 (22.3\%)                        \\ \hline
		\textbf{2x}            & 207 (29.6\%)                     & 97 (13.9\%)                                   & 20 (2.9\%)                          \\ \hline
		\textbf{1.5x}          & 254 (36.3\%)                     & 127 (18.1\%)                                  & 24 (3.4\%)                          \\ \hline
		\textbf{1.2x}          & 256 (36.6\%)                     & 251 (35.9\%)                                  & 77 (11.0\%)                         \\ \hline
		\textbf{1.1x}          & 321 (45.9\%)                     & 351 (50.1\%)                                  & 160 (22.9\%)                        \\ \hline
	\end{tabular}
	\egroup
	\caption{Number of times each selection strategy selected the slower function (out of 700 counted selections).}
	\label{tab:strategy_comparison}
\end{table}

The baseline solution for this test would be a round-robin strategy that would run the wrong function 350 times out of 700 times, having a success rate of 50\%. 

As we can see, even though the function complexity is linear, the linear regression has a 15\% error rate on a 4 times slower function. The window-bound linear regression has a lot better results on a significantly slower function (the $k=2$ and $k=4$ cases). Both the linear regression and the window-bound linear regression fail on the function with smallest difference, most likely because of the significance level interval testing. Adjusting $\alpha$ might increase the success rate in these cases. The local regression is the clear winner in all the cases except for the first one, where the model on one of the 5 sequences made 139 wrong decisions out of 140 calls in total. This demonstrates a complete failure of the model and a problem caused by not having any tests for certainty incorporated in the strategy.

In conclusion, the overall best results are provided by the local regression strategy, although it is a little error-prone. The window-bound linear regression is worse, but still usable, especially in case of more significant differences between the functions.

\subsection{Strategy performance}
\label{subsec:strategy_perf}

When deciding for one of the strategies, there is another concern that we should keep in mind. The strategies tend to be quite complex and their execution time might become non-trivial. As they need to process the entire run history of each of the functions, their selection times increase over time, as more historical measurements are collected.

For some of the strategies, it is possible to compute and cache the statistical data in advance, at the moment of storing newly measured data, allowing them to avoid processing all the records again during the selection. From our strategy list, this is possible to be done for the linear regression and the t-tests. Unfortunately, neither the window-bound strategies, nor the local regression support this approach.

To compare the practical overheads, we ran the selection using each of the strategies 20000 times in a row (while evaluating the runs and adding new data in the process). The window-bound strategies had the window size calculated to contain 25 records in average. The resulting average selection strategy run times from samples collected every 50 runs are in table \ref{tab:strategy_execution_comparison}.

\begin{table}[h!]
	\captionsetup{justification=centering,margin=0.5cm}
	\centering
	\bgroup
	\def\arraystretch{1.5}%
	\begin{tabular}{|l|r|r|}
		\hline
		& \multicolumn{1}{l|}{\textbf{Not cached}} & \multicolumn{1}{l|}{\textbf{Cached}} \\ \hline
		\textbf{Linear regression} & 3.31                                   & 0.35                                 \\ \hline
		\textbf{Window-bound LR}   & 1.29                                   & N/A                                  \\ \hline
		\textbf{LOESS}             & 136.43                                 & N/A                                  \\ \hline
		\textbf{T-test}            & 2.25                                   & 0.21                                 \\ \hline
	\end{tabular}
\egroup
\caption{Average strategy selection time (in ms) in a sequence of 20000 consecutive selections.}
\label{tab:strategy_execution_comparison}
\end{table}

The results are showing that local regression has extremely high run times that can grow above 100ms with larger numbers of run history records. An overhead of around 100ms on every selection would be too high for most common usages, so the local regression strategy, even though it has the best success rate, should be used with extreme caution and only in cases where it is clear that overhead this high cannot damage the system performance. As for the remaining strategies, it seems that caching speeds up the selection about 10 times. The window-bound linear regression cannot be improved by the caching (as the regression done for each input is potentially different), but has the lowest execution time of all the not cached strategies, and therefore is quite usable.

\section{Invocation policies}

The selection process and all the strategies described in sections \ref{sec:input_based_strategies} and \ref{sec:mean_based_strategies} are quite complex and have non-trivial overhead, especially after collecting a large amount of historical data, as discussed in section \ref{subsec:strategy_perf}. In a common scenario where we expect the system to come to a decision about the best function (either overall, or for every group of \textit{input descriptors}), it would be handy if we could stop the selection process at a certain point and keep using only the most favored function with no extra overhead. Or, to use it most of the time, with only occasional attempts at the selection in order to detect possible changes in the system (response times, I/O operation duration, etc.).

Similarly, it might be desirable to control the overhead time spent on the selection, or, in general, on invoking the functions using selection (which might lead to a bad decision), either per a specific time period, or per a specific action of the system (user request, processing unit, etc.). All of these decisions are taken at a higher level than the run time history analysis performed by the selection strategies.  

\textit{Invocation policies} are a concept introduced into the ScalaAdaptive framework that tries to address these cases, with the main purpose of lowering its execution overhead. Every \textit{combined function} has an \textit{invocation policy} associated with it. In the function selection chain, the \textit{policy} gets evaluated right after invoking the \textit{combined function} and is supposed to quickly decide how to proceed with the invocation. There might be scenarios where a faster ways could be taken without even using any of the strategies and analyzing the historical data.

The possible results of the \textit{policy} evaluation are the following:

\begin{itemize}
	\item \textbf{SelectNew} - Select function using the selection strategy, executing the chain described in section \ref{sec:selection_and_invocation_process}
	\item \textbf{GatherData} - Gather more data for the least-executed function (function selection is skipped, but the run time is measured and the data is stored to the history)
	\item \textbf{UseLast} - Use the function selected last time (without measuring run time and storing it to the history)
	\item \textbf{UseMost} - Use the most selected function (without measuring run time and storing it to the history)
\end{itemize}

We can notice that these results require only the basic statistical data about previous selection processes. This is the key idea of the \textit{policies} - the decision and the execution should depend only on these simple statistics, except for the case where selection strategy is required.

The \textit{policy} evaluation process also replaces the current \textit{policy} with a new one. This technically creates a state machine - the state of the function is represented by the active \textit{policy}, and whenever the function is invoked, a result is produced and new \textit{policy} becomes active. This corresponds to producing output and moving to a new state in a state machine.

The advantage of the \textit{policy} system over a regular state machine is its extensibility and reusability - there are no rules directly in the state machine (i.e. here in the \textit{combined function}). All the logic of deciding on the result and the next \textit{policy} is in the \textit{policy} node itself. So the entire behavior of the state machine is defined by the initial \textit{policy}, which should be specified by the user upon creating the \textit{combined function}. As a result, reusable policies that are parametrized can be used in various chains of policies, smaller chains can be put together, etc.
%TODO: Reference to the API

\subsection{Statistical data}
\label{subsec:statistical_data}

The only input that the \textit{policy} receives when making decision is a statistical summary of previous selection results of the \textit{combined function}. The process itself is supposed to be fast and reflect just the trends in the selection, not to replace the whole selection strategy process as described in sections \ref{sec:input_based_strategies} and \ref{sec:mean_based_strategies}, so the statistical data do not contain the entire run history of all the functions involved.

\begin{figure}[h!]
	\captionsetup{justification=centering,margin=0.5cm}
	\centerline{\mbox{\includegraphics[width=110mm]{./img/run_schema.png}}}
	\caption{Process of \textit{combined function} invocation.}
	\label{fig:run_schema}
\end{figure}

On the other hand, the \textit{policies} are a suitable tool to limit the potential damage of the selection process on the system. For this, it is necessary to observe and to store various times connected with the selection and execution. Figure \ref{fig:run_schema} shows the whole process with policy being evaluated with the SelectNew result, split into individual parts. Upon every invocation, the following is tracked:

\begin{itemize}
	\item Selection and run history storage time (2 and 4)
	\item Function execution time (3)
	\item Total time spent on the SelectNew result (2, 3 and 4)
	\item Total time spent on the GatherData result (3 and 4, 2 is skipped in such a case)
\end{itemize}

Sums of these times are kept in every \textit{combined function}. The policy can work with these totals and analyze the changes that happen to them overtime. The policies are designed to be immutable, but contain state themselves, and the state changes when switching to a new policy.

In addition, number of times each function was selected, total number of times we received the SelectNew and GatherData result and the total number of times the combined function was invoked has to be tracked. The user-implemented policy has the following data available to decide:

\begin{itemize}
	\item Total run count of the combined function
	\item Total number of times the last function was selected
	\item Total number of times the most selected function was selected
	\item Total number of times any function was selected
	\item Total number of times of gathering new data (GatherData result)
	\item Total time spent on function execution (after SelectNew result)
	\item Total time spent on selection and storage overhead (after SelectNew result)
	\item Total time spent on processing the SelectNew result
	\item Total time spent on processing the GatherData result
\end{itemize}

\subsection{Implemented policies}

As mentioned earlier, \textit{policies} are designed to be reusable as a building-blocks. The immutable \textit{policy} itself is the only holder of the state and change of state can be performed only by transitioning to a different \textit{policy}. Its functionality and very simple decision-making process can be visualized using a transition diagram, which shows the \textit{policy} itself as a red square, all the other \textit{policies} as a gray square and green arrows as state transitions. The transitions might be conditional, in such a case, the condition splits the arrow into two. The transition can produce a result which is shown next to the transition arrow. If a result is produced, the \textit{policy} makes a decision and next \textit{policy} will be evaluated during the following invocation. If a result isn't produced, the \textit{policy} just delegates the decision to the next \textit{policy} which is evaluated immediately. The chain of evaluation stops when a result is produced.

The transition conditions use \textit{policy} parameters, which are passed to a \textit{policy} in the constructor, and the current statistic data explained in section \ref{subsec:statistical_data}. It can also use static data and methods that it has access to. The parameters of the top-level (starting) \textit{policy} are passed in by the user creating it, the parameters of the other policies in the chain are fixed when the \textit{policies} are created, usually during the decision of the top-level \textit{policy}, which typically builds the entire policy chain (that can be cyclic and lead back to it).

The transition diagrams and short descriptions of some \textit{policies} will follow.

\subsubsection{Building blocks}

\begin{figure}[h!]
	\captionsetup{justification=centering,margin=0.5cm}
	\centerline{\mbox{\includegraphics[width=110mm]{./img/helper_policies.png}}}
	\caption{Transition diagram of the basic building-block policies.}
	\label{fig:helper_policies}
\end{figure}

Figure \ref{fig:helper_policies} shows the main policies that serve as a building-blocks. The most important is the \textbf{Repeat until \textit{condition}} \textit{policy}, parametrized by a \textit{condition}, result \textit{R}, which it keeps producing until the \textit{condition} becomes true, and the policy \textit{P}, which it makes transition to at that moment. The \textbf{Always do} \textit{policy} represents an infinite loop producing the same result \textit{R} every time, and the \textbf{Do once} \textit{policy} produces the result \textit{R} once and immediately makes transition to the policy \textit{P}.

\begin{figure}[h!]
	\captionsetup{justification=centering,margin=0.5cm}
	\centerline{\mbox{\includegraphics[width=75mm]{./img/start_policy.png}}}
	\caption{Transition diagram of the \textbf{Start policy}.}
	\label{fig:start_policy}
\end{figure}

Figure \ref{fig:start_policy} represents the \textbf{Start policy}, which accepts the gather frequency \textit{GF}, number of invocation times before moving to a next policy \textit{N}, and the policy itself \textit{P}. It is a little more complicated and specialized version of the \textbf{Repeat until} policy - it keeps producing the SelectNew result, and every \textit{GF}-th invocation, it produces the GatherData result. It might be useful at the beginning, when it is necessary to gather data for all of the functions and perform some selections so that the following policies had statistical data to base their decision on.

\subsubsection{Stop selecting when decided}

\begin{figure}[h!]
	\captionsetup{justification=centering,margin=0.5cm}
	\centerline{\mbox{\includegraphics[width=75mm]{./img/stop_selecting_when_decided.png}}}
	\caption{Transition diagram of the \textbf{Stop selecting  when decided} policy.}
	\label{fig:stop_selecting_when_decided}
\end{figure}

In order to limit the overhead, it might be useful to completely stop selecting when we are sure that one of the functions gets selected a lot more often. The \textbf{Stop selecting when decided} policy shown in figure \ref{fig:stop_selecting_when_decided} keeps selecting new function until both the total number of invocations exceeds minimum run count \textit{MR} and the rate of the number of times that the last selected function was selected and the total number of invocations exceeds the minimum percentage \textit{MP}. After that, it will keep using the last function forever. 

The advantage of falling back to the infinite \textbf{Always use last} policy is its basically zero overhead - no condition is evaluated in the decision process. So this policy is useful for the fast functions where keeping low overhead for the long-term is critical. It cannot however, undo a wrong decision, reflect a change in conditions, or anything else.

\subsubsection{Pause selection after streak}

\begin{figure}[h!]
	\captionsetup{justification=centering,margin=0.5cm}
	\centerline{\mbox{\includegraphics[width=85mm]{./img/pause_selection_after_streak.png}}}
	\caption{Transition diagram of the \textbf{Pause selection after streak} policy.}
	\label{fig:pause_selection_after_streak}
\end{figure}

To lower the number of selections, an approach based on the streak lengths can be used as well. It is implemented by the \textbf{Pause selection after streak} policy in figure \ref{fig:pause_selection_after_streak}. Its functionality is defined by the \textit{SL} (streak length) and \textit{RE} (retry every) arguments. Whenever one function is selected \textit{SL} times in a row, the selection process stops and the UseLast result is being produced. Once every \textit{RE} times, a new selection attempt is done, looping back to the original policy. If the same function is selected once again, the streak gets longer by one and the selection process stops again. If, however, a different function is selected this time, the streak becomes zero and we start selecting again.

The described behavior is the preferred way of limiting the selection overhead. By setting the streak length to a reasonable number (20-100, based on the data, situation, etc.), we can assure that if one function is significantly better than the others, it will be used without the need to select it. The retry that happens every once in a while makes sure that eventual wrong decision will be detected and will reset the system back to the beginning.

\subsubsection{Limited overhead, gather time and selection time}

\begin{figure}[h!]
	\captionsetup{justification=centering,margin=0.5cm}
	\centerline{\mbox{\includegraphics[width=85mm]{./img/limited_overhead.png}}}
	\caption{Transition diagram of the \textbf{Limited overhead} policy.}
	\label{fig:limited_overhead}
\end{figure}

Policies can be used to limit the framework involvement per a real-time period, so that the selection overhead, or overhead generated by gathering data or wrongly selecting the slower function doesn't damage the system more than a specified limit. The \textbf{Limited overhead} policy in figure \ref{fig:limited_overhead} limits only the selection overhead time to \textit{AO} (allowed overhead) every \textit{RP} (reset period). After depleting the allowed time, it cannot select anymore and keeps using the most selected function until the end of the period.

\begin{figure}[h!]
	\captionsetup{justification=centering,margin=0.5cm}
	\centerline{\mbox{\includegraphics[width=85mm]{./img/limited_gather_time.png}}}
	\caption{Transition diagram of the \textbf{Limited gather time} policy.}
	\label{fig:limited_gather_time}
\end{figure}

In exactly the same way, the time spent on executing the function for the result GatherData and the whole process of selection and execution for the SelectNew result can be limited. The figure \ref{fig:limited_gather_time} demonstrates the \textbf{Limited gather time} policy, which keeps producing GatherData until total gather time reaches \textit{AGT} (allowed gather time), after that, it continues with the SelectNew result until total selection time reaches \textit{AST} (allowed selection time). A loop producing UseMost follows, for the rest of the real-time period specified by \textit{RP} (reset period).

This \textit{policy} gives us control over the time spent on selecting results in real-time. If we get a lot of requests at the same time, receive a large batch of data to process or for any reason need to momentarily increase the throughput of the system, the time or overhead limits get depleted very quickly and the UseMost fallback result means reasonably good performance with a minimum overhead.

\subsection{Policy builder}

As we could see from the policy examples, most of them were built only using the \textbf{Repeat until} or \textbf{Do once} blocks in a simple cycle. This process of combining these generic blocks into sequences can be automatized and hidden behind a simple DSL (see \ref{sec:dsls}) that would allow the user to express his customized way to decide about the function run. The DSL was implemented and named \textit{policy builder}.

Figure \ref{fig:policy_builder_chart} shows the syntax diagram of \textit{policy builder}.

\begin{figure}[h!]
	\captionsetup{justification=centering,margin=0.5cm}
	\centerline{\mbox{\includegraphics[width=130mm]{./img/policy_builder_chart.png}}}
	\caption{Syntax diagram of the \textit{policy builder} DSL.}
	\label{fig:policy_builder_chart}
\end{figure}

The policy built with \textit{policy builder} has always one main loop. Its definition starts with the first result that it emits: \textit{selectNew}, \textit{gatherData}, \textit{useMost} or \textit{useLast}. A result is always followed by a duration specifier, and another result. When we specify the sequence of results and their durations, we can end it by either closing the loop (using \textit{andThenRepeat}), by transitioning to an external policy (using \textit{andThenGoTo}, leads to a non-looped policy), or by using \textit{forever} as a duration for the last policy, which limits the loop to it. In addition, it is possible to put a conditional transition to a third party policy anywhere in between two results (using \textit{andThenIf}).

The conditions can be either simple functions accepting statistical data and returning a boolean value, or a \textit{growsBy} condition with a value extractor (a function that extracts a value of some type from the statistical data) and an amount. A simple condition is evaluated only once against current statistical data. In case of the \textit{growsBy} condition, the extractor is executed once at the beginning of the policy loop and its result is stored, and the condition evaluation consist in executing it and comparing current result to the stored one.

An example of a simple policy described using the \textit{policy builder} follows. It produces GatherData 50 times, then SelectNew 50 times, and then either loops forever on UseLast, if the streak is larger than 20, or repeats the whole process otherwise.

\lstset{style=Scala}
\begin{lstlisting}
val useLastForever: Policy = useLast forever
val conditional: Policy = (
  gatherData until (totalRunCount growsBy 50)
  andThen selectNew until (totalRunCount growsBy 100)
  andThenIf ((stats: StatisticDataProvider) => stats.getStreakLength >= 20) goTo useLastForever
  andThenRepeat
)
\end{lstlisting}

\subsection{Policies and groups}

One of the techniques for improving the selection process is grouping the history records using some input features, as described in \ref{subsec:grouping}. We will extend the grouping to function statistics and to the current policy. It means that the \textit{combined function} needs to hold the policy and the statistical data for every group and to evaluate and update the 

The main advantage is the possibility to take faster decisions in some groups, using for example the \textit{Pause selection after streak} policy. When a function is significantly faster than the others for some category of inputs, the streak will build up fast within the group and the invocation for future inputs from such a group will require no selection and will be very quick, yielding correct results.

\subsection{Possible improvements}
\label{subsec:policy_improvements}

Part of the statistical data could be a factor of combined certainty of all the selections of a specific function. This factor could be aggregated from the selection results of the strategies that would support it (in the statistical test based strategies, it could be a p-value, strength of the test, or similar) and would serve as a decision factor for more complex policies with a statistical approach as well.

The \textit{policies} were designed as a concept that is completely independent from the actual selection strategies and history analysis, to be faster and reusable. One of the improvements that would require closer tying would be to allow the \textit{policies} to choose the selection strategy, or to influence it somehow, by limiting it to a subset of data, etc. 

Another improvement that might seem useful is to allow the \textit{policies} to analyze the whole history data. This would, however, mean a lot of additional overhead time when evaluating the \textit{policy} and is not desirable.
With a different approach, we could achieve the same thing by giving selection strategies a state that could involve their decision. The state would be managed by the combined function and it could even be the same state that is used by the \textit{policies}. 

%TODO - when do I need more data?

\section{Summary}


\begin{algorithmic}[1] % The number tells where the line numbering should start
	\State $(result, newPolicy) \gets evaluatePolicy(currentPolicy)$
	\State $currentPolicy \gets newPolicy$
	\Switch{$result$}
	\Case{UseLast}
	\State $f \gets getLast()$
	\State \Return{$f()$}
	\EndCase
	\Case{UseMost}
	\State $f \gets getMost()$
	\State \Return{$f()$}
	\EndCase
	\EndSwitch
	\For{$i = 1$ to $n$}
	\State $d_i \gets getHistoricalData(d_i)$
	\EndFor	
	\Switch{$result$}
	\Case{GatherData}
	
	\EndCase
	\EndSwitch
	\If{not $equalityRejected$}
	\State use secondary selector
	\EndIf
	\If {$mean(data_1) < mean(data_2)$}
	\State select $f_1$
	\Else 
	\State select $f_2$
	\EndIf
\end{algorithmic}
